# the original step3: train and test the phenotype confirmation model for rare disease entity linking filtering.
# updated with functions and for parameter tuning
# the program uses the output of mention_disamb_data files generated by step1_tr_data_creat_ment_disamb.py

from sent_bert_emb_viz_util import load_data, encode_data_tuple, get_model_from_encoding_output, test_model_from_encoding_output, output_to_file
import random

import pickle

from evaluation_util import get_and_display_results,rule_based_model_ensemble
from sklearn.metrics import precision_score, recall_score, f1_score
import numpy as np
import pandas as pd
import os

if __name__ == '__main__':
    dataset='Tayside' # 'MIMIC-III' or 'Tayside'
    data_category = 'Radiology' # 'Radiology' or 'Discharge summary'
    if dataset=='Tayside':
        assert data_category=='Radiology' # the report type should be radiology for Tayside data
        
    #thresholds range for parameter tuning    
    #there are two parameter tuning paths: 
    #   (i) tune "prevalence" threshold and mention length threshold together, by setting window_size as 5 (tune_window_size_only as False). 
    #   (ii) tune window_size by setting "prevalence" threshold and mention length threshold as the best values (tune_window_size_only as True). 
    tune_window_size_only = False #set as False for radiology reports as we don't tune the paramter for this.
    
    #prevalence_percentage_threshold = 0.05 # "prevalence" threshold, p
    #in_text_matching_len_threshold = 3 # mention length threshold, l
    prevalence_percentage_threshold_range = [0.0001,0.0005,0.001,0.005,0.01,0.05,0.1] if not tune_window_size_only else [0.005]
    in_text_matching_len_threshold_range = [2,3,4] if not tune_window_size_only else [3]
    window_size_range = [5] if not tune_window_size_only else [3,5,10,15,50]
    #here needed to be set
    masked_training = False
    use_doc_struc = False

    #num_sample = 100000 # for paramter tuning only #len(data_list_tuples) #500 #len(data_list_tuples) #1000 for quick checking of the program

    # data selection approaches: random sampling, balanced random sampling, and diverse sampling
    num_of_training_samples_non_masked = 100000 # much more than the whole data, i.e. using the whole data # 9000
    #balanced_random_sampling=True
    balanced_random_sampling_range=[True,False]
    diverse_sampling=False
    num_of_data_per_mention=25 # for diverse sampling only
    masking_rate = 1 #0.15 # to implement
    #window_size = 5
    C = 1 # l2 regularisation parameter
    num_of_testing_samples = 400
    
    #on windows
    #model_path='C:\\Users\\hdong3\\Downloads\\pubmedBERT_tf_model_new\\microsoft\\'; model_name = 'pubmedBERT' # coverted to tf from https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/tree/main
    #model_path='C:\\Users\\hdong3\\Downloads\\NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12\\'; model_name = 'blueBERTnorm'
    #model_path='C:\\Users\\hdong3\\Downloads\\NCBI_BERT_pubmed_mimic_uncased_L-24_H-1024_A-16\\'; model_name = 'blueBERTlarge'
    
    #on Eddie
    #model_path='/exports/cmvm/eddie/smgphs/groups/hdong3-res/Rare-disease-identification/pubmedBERT_tf_model_new/microsoft/'; model_name = 'pubmedBERT'
    #model_path = '/exports/cmvm/eddie/smgphs/groups/hdong3-res/Rare-disease-identification/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12/'; model_name = 'blueBERTnorm'
    #model_path = '/exports/cmvm/eddie/smgphs/groups/hdong3-res/Rare-disease-identification/NCBI_BERT_pubmed_mimic_uncased_L-24_H-1024_A-16/'; model_name = 'blueBERTlarge'
    
    #on crypt
    model_path = './NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12/'; model_name = 'blueBERTnorm'
    
    for prevalence_percentage_threshold in prevalence_percentage_threshold_range:
        for in_text_matching_len_threshold in in_text_matching_len_threshold_range:
            for balanced_random_sampling in balanced_random_sampling_range:
                for window_size in window_size_range:
                    print('p%s l%s balanced_random_sampling %s' % (str(prevalence_percentage_threshold),str(in_text_matching_len_threshold),'True' if balanced_random_sampling else 'False'))
                    #construct filename of data_list_tuples for training - the output of step1
                    data_list_tuples_pik_fn = 'mention_disamb_data%s%s%s%s.pik' % ('' if dataset == 'MIMIC-III' else '_Tayside','-rad' if data_category == 'Radiology' else '','_p%s' % str(prevalence_percentage_threshold) if prevalence_percentage_threshold != 0.005 else '','_l%s' % str(in_text_matching_len_threshold) if in_text_matching_len_threshold != 3 else '')
                    
                    #construct filename for trained models             
                    #trained_models_name = 'model_%s_ws%s%s%s%s.pik' % (model_name, str(window_size), '_ds' if use_doc_struc else '', '_divs%s' % str(num_of_data_per_mention) if diverse_sampling else '','_nm%sm%s' % (str(num_of_training_samples_non_masked),str(num_of_training_samples_masked)))
                    #print(trained_models_name)

                    #1. load data, encoding, and train model 
                    #load data
                    #data_list_tuples = load_data('mention_disamb_data.pik')
                    if os.path.exists(data_list_tuples_pik_fn):
                        data_list_tuples = load_data(data_list_tuples_pik_fn)
                    else:
                        # continue the loop if the data_list_tuples_pik_fn does not exist (this is because there are less than 2 classes in the data - filtered out in step1_tr_data_creat_ment_disamb.py)    
                        print('file not exist due to single or no classes for this data parameter setting for binary classification')
                        continue
                    random.Random(1234).shuffle(data_list_tuples) #randomly shuffle the list with a random seed
                    #if num_sample > len(data_list_tuples):
                    #    num_sample = len(data_list_tuples)
                    #data_list_tuples = data_list_tuples[0:num_sample] # set a small sample for quick testing
                    print(len(data_list_tuples))

                    print('start encoding')
                    
                    store_cw_encoding_and_men_tokens_dicts = False if window_size >= 15 else True # not saving too big files
                    
                    #encoding
                    #marking_str_tr = 'training_%s%s%s' % (str(num_sample),'_p%s' % str(prevalence_percentage_threshold) if prevalence_percentage_threshold != 0.005 else '','_l%s' % str(in_text_matching_len_threshold) if in_text_matching_len_threshold != 3 else '')
                    marking_str_tr = 'training%s%s%s%s' % ('' if dataset == 'MIMIC-III' else '_Tayside','_rad' if data_category == 'Radiology' else '','_p%s' % str(prevalence_percentage_threshold) if prevalence_percentage_threshold != 0.005 else '','_l%s' % str(in_text_matching_len_threshold) if in_text_matching_len_threshold != 3 else '')
                    output_tuple_encoded = encode_data_tuple(data_list_tuples, masking=masked_training, with_doc_struc=use_doc_struc, model_path=model_path, marking_str=marking_str_tr, window_size=window_size, masking_rate=masking_rate, diverse_sampling=diverse_sampling, num_of_data_per_mention=num_of_data_per_mention,store_cw_encoding_and_men_tokens_dicts=store_cw_encoding_and_men_tokens_dicts)
                        
                    #training
                    clf_model,train_binary_class_dist_str,train_and_valid_reports_report = get_model_from_encoding_output(output_tuple_encoded,num_of_training_samples_non_masked,balanced_sampling=balanced_random_sampling,report_binary_class_dist_output=True,report_train_validation_results=True)

                    #export models
                    #with open(trained_models_name, 'wb') as data_f:
                    #    pickle.dump((clf_model_non_masked_ds,clf_model_masked), data_f)
                    #    print('\n' + trained_models_name, 'saved')

                    #2. load testing data and predict results: 
                    if data_category == 'Discharge summary':
                        marking_str_te = 'testing_%s' % str(len(data_list_tuples))
                        evaluation_data_sheet_fn = 'for validation - SemEHR ori.xlsx'
                    elif data_category == 'Radiology':
                        marking_str_te = 'testing_198_MIMIC-III_rad' if dataset == 'MIMIC-III' else 'testing_279_Tayside_rad'
                        evaluation_data_sheet_fn = 'for validation - 1000 docs - ori - MIMIC-III-rad.xlsx' if dataset == 'MIMIC-III' else 'for validation - 5000 docs - ori - tayside - rad.xlsx'
                    #load data from .xlsx and save the results to a specific column
                    # get a list of data tuples from an annotated .xlsx file
                    # data format: a 6-element tuple of section_text, document_structure_name, mention_name, UMLS_code, UMLS_desc, label (True or False)
                    df = pd.read_excel(evaluation_data_sheet_fn)
                    # change nan values into empty strings in the two rule-based label columns
                    #df[['neg label: only when both rule 0','pos label: both rules applied']] = df[['neg label: only when both rule 0','pos label: both rules applied']].fillna('') 
                    # if the data is not labelled, i.e. nan, label it as -1 (not positive or negative)
                    #df[['manual label from ann1']] = df[['manual label from ann1']].fillna(-1) 

                    data_list_tuples = []
                    for i, row in df.iterrows():
                        #filter out the manually added rows that were created during the annotation
                        if 'manually added data' in row:
                            if not pd.isna(row['manually added data']):
                                print(str(row['manually added data']))
                                continue
                        doc_struc = row['document structure']
                        text = str(row['Text'])
                        mention = row['mention']
                        UMLS_code = str(row['UMLS with desc']).split()[0]
                        UMLS_desc = ' '.join(str(row['UMLS with desc']).split()[1:])
                        #label = row['manual label from ann1']
                        label = row['gold text-to-UMLS label']
                        label = 0 if label == -1 else label # assume that the inapplicable (-1) entries are all False.
                        #print(label)
                        data_tuple = (text,doc_struc,mention,UMLS_code,UMLS_desc,label)
                        #if i<2:
                        #    print(data_tuple)
                        data_list_tuples.append(data_tuple)

                    # get testing data rep and predict with the model
                    # encoding
                    output_tuple_test_encoded = encode_data_tuple(data_list_tuples, masking=masked_training, with_doc_struc=use_doc_struc, model_path=model_path, marking_str=marking_str_te, window_size=window_size, masking_rate=masking_rate)

                    # prediction
                    print('single model results')
                    print('%s training%s:' % ('masked' if masked_training else 'non-masked', ' with ds' if use_doc_struc else ''))
                    y_test_labelled, y_pred_test_labelled,list_of_err_samples_non_masked = test_model_from_encoding_output(output_tuple_test_encoded, num_of_testing_samples, clf_model)
                    prec, rec, f1, tn, fp, fn, tp = get_and_display_results(y_test_labelled, y_pred_test_labelled, report_confusion_mat=True)
                    #also returned the list of erroneous samples using the non-masked encoding.
                    
                    # save results to .txt
                    filename_result_report = 'results%s%s_%s%s%s%s%s.txt' % ('' if dataset == 'MIMIC-III' else '_Tayside','_rad' if data_category == 'Radiology' else '',model_name,'balanced' if balanced_random_sampling else '','_p%s' % str(prevalence_percentage_threshold),'_l%s' % str(in_text_matching_len_threshold),'_cw%s' % window_size if window_size !=5 else '')
                    
                    train_data_stat_report = 'train data stat - p%s, l%s\n' % (str(prevalence_percentage_threshold),str(in_text_matching_len_threshold)) + train_binary_class_dist_str
                    result_report = train_and_valid_reports_report + '\ntest precision: %s\n test recall: %s\n test F1: %s\n' % (str(prec), str(rec), str(f1))
                    result_report = result_report + 'tp %s\n tn %s\n fp %s\n fn %s' % (str(tp),str(tn),str(fp),str(fn))
                    output_to_file(filename_result_report,train_data_stat_report + '\n\n' + result_report)